{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5902c1f3",
   "metadata": {},
   "source": [
    "<div dir=rtl align=center>\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/fa/thumb/a/a9/Sharif_logo.svg/626px-Sharif_logo.svg.png?20110526112825' alt=\"SUT logo\" width=200 height=200 align=center  >\n",
    "<br>\n",
    "<font face=\"B Yekan\">\n",
    "<font color=0F5298 size=7>\n",
    "یادگیری ماشین<br>\n",
    "<font color=2565AE size=5>\n",
    "دانشکده مهندسی صنایع<br>\n",
    "<font color=2565AE size=4>\n",
    "دکتر مهدی شریف زاده <br>\n",
    "<font  size=4>\n",
    "\n",
    "امیرحسین محمودی\n",
    " <br>\n",
    "بهار 1402<br>\n",
    "\n",
    "<font color=3C99D size=5>\n",
    "تمرین عملی 2\n",
    "<br>\n",
    "سوال 2\n",
    "- تشخیص هرزنامه ها\n",
    "<br>\n",
    "200 نمره\n",
    "    \n",
    "    \n",
    "____\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e85f16a",
   "metadata": {},
   "source": [
    "### Full Name : \n",
    "\n",
    "### Student Number : \n",
    "\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac560d39",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"font-size:16px;\">\n",
    "<font face=\"B Yekan\">\n",
    "در این تمرین می خواهیم مدلی ایجاد کنیم تا پیام های هرز را از پیام های معمولی تشخیص دهیم.\n",
    "داده های داده شده حاوی دو ستون هستند .یکی ستون متن پیام و دیگری برچسب پیام که Spam نشان دهنده هرز بودن پیام و ham نشان دهنده پاک بودن پیام است.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec5c2f08",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de2d1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e104a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import numpy as np # linear algebra\n",
    "import os # accessing directory structure\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# add what you want"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "437f821b",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2e2b420",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"font-size:16px;\">\n",
    "<font face=\"B Yekan\">\n",
    "داده ها را در فرمت مناسب زبان فارسی بخوانید\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfff8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5de92e51",
   "metadata": {},
   "source": [
    "# A. Exploratory Analysis (10 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5614eb34",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"font-size:16px;\">\n",
    "<font face=\"B Yekan\">\n",
    "\n",
    "در صورتی که داده شما شامل مقادیر خالی است آن ها را حذف کنید.\n",
    "\n",
    "یک ستون دیگر به داده ها اضافه کنید و طول هر پیام را در آن درج کیند.\n",
    "\n",
    "نمودار تعداد پیام های هرز و غیر هرز برحسب طول پیام را رسم کنید.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ff5df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e8f94f6",
   "metadata": {},
   "source": [
    "# B. Text Cleaning (30 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80cff199",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"font-size:16px;\">\n",
    "<font face=\"B Yekan\">\n",
    "\n",
    "حال نیاز است تا [واژه های پالایشی](https://fa.wikipedia.org/wiki/%D9%88%D8%A7%DA%98%D9%87_%D9%BE%D8%A7%D9%84%D8%A7%DB%8C%D8%B4%DB%8C_(%D9%BE%D8%B1%D8%AF%D8%A7%D8%B2%D8%B4_%D8%B2%D8%A8%D8%A7%D9%86_%D8%B7%D8%A8%DB%8C%D8%B9%DB%8C))\n",
    "(stop words) \n",
    "را به همراه بعضی علامت ها از متن ها حذف کنیم.\n",
    "برای حذف علائم می توانید از کتابخانه string  استفاده کنید.\n",
    "\n",
    " می توانید فهرست کلمات پالایشی انگلیسی \n",
    " ( با استفاده از  کتابخانه NLTK) \n",
    " همراه با \n",
    "  فهرست کلمات پالایشی زبان فارسی  که در پوشه تمرین قراره داده شده است \n",
    "([منبع و اطلاعات بیشتر](https://github.com/ziaa/Persian-stopwords-collection))\n",
    " استفاده کنید.\n",
    "\n",
    " تابع زیر را به شکلی تکمیل کنید که با گرفتن یک متن کلمات پالایشی و علائم را از متن حذف کند.\n",
    " \n",
    " حال این تابع را روی داده های خود اجرا کنید و نتیجه را در ستونی به نام text_new ذخیره کنید.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954dcf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "#TODO\n",
    "\n",
    "#remove the punctuations and stopwords\n",
    "\n",
    "def text_process(text):\n",
    "#TODO\n",
    "\n",
    " pass\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb466da5",
   "metadata": {},
   "source": [
    "# C. Word Cloud (30 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5b96308",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"font-size:16px;\">\n",
    "<font face=\"B Yekan\">\n",
    "حال می خواهیم ابر کلمات هر دسته را رسم کنیم تا کلمات کلیدی هرزنامه ها را به صورت چشمی بررسی کنیم.\n",
    "\n",
    "برای اینکار ابتدا با مفهوم [tokenization](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization) آشنا می شویم.\n",
    "در واقع نیاز داریم تا جملات را به یک سری token (مفهومی شبیه به کلمه)\n",
    "بشکنیم.\n",
    "\n",
    "برای ساخت ابر کلمات نیاز است تا هر جمله راه به token هایی تبدیل کنیم.\n",
    "سپس تمام token های پیام های هرز  را در یک رشته با فاصله کنار هم قرار دهیم.\n",
    "همین کار را برای دسته پیام های پاک نیز انجام می دهیم.\n",
    "حال با استفاده از این رشته ها می توان ابر کلمات هر دسته را ایجاد کرد.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b25cfd2",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"font-size:16px;\">\n",
    "<font face=\"B Yekan\">\n",
    "جهت تبدیل جملات به token  میتوان از ابزار های کتابخانه NLTK استفاده کرد.\n",
    "\n",
    "اما ما برای بالا بردن دقت کار از کتابخانه [هضم](https://www.roshan-ai.ir/hazm/docs/)\n",
    " که برای زبان فارسی توسعه داده شده است استفاده می کنیم.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bb16d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f2c11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import word_tokenize\n",
    "# Import nltk packages and Punkt Tokenizer Models\n",
    "# import nltk\n",
    "# nltk.download(\"punkt\")\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965cbde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab882d85",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"font-size:16px;\">\n",
    "<font face=\"B Yekan\">\n",
    "برای رسم ابر کلمات به زبان انگلیسی می توان از کتابخانه WordCloud استفاده کرد.این کتابخانه از زبان فارسی پشتیبانی نمی کند\n",
    "\n",
    "برای رسم ابر کلمات در زبان فارسی از کتابخانه  [WordCloud_fa](https://pypi.org/project/wordcloud-fa/)\n",
    "استفاده می کنیم.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea0dc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install wordcloud-fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64a0db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud_fa import WordCloudFa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efc43801",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"font-size:16px;\">\n",
    "<font face=\"B Yekan\">\n",
    "ابر کلمات دو دسته هرز و غیر هرز را رسم کنید\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbf1c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d82c3e5",
   "metadata": {},
   "source": [
    "# D. Vectorization (30 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0987bd0",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"font-size:16px;\">\n",
    "<font face=\"B Yekan\">\n",
    "برای استفاده از جملات برای یادگیری ماشین نیاز است تا آن ها را به شکل برداری تبدیل کنیم.برای این کار روش های مختلفی وجود دارد.\n",
    "\n",
    "\n",
    " ما در اینجا از روش  [فراوانی وزنی تی‌اف-آی‌دی‌اف](https://fa.wikipedia.org/wiki/%D9%81%D8%B1%D8%A7%D9%88%D8%A7%D9%86%DB%8C_%D9%88%D8%B2%D9%86%DB%8C_%D8%AA%DB%8C%E2%80%8C%D8%A7%D9%81-%D8%A2%DB%8C%E2%80%8C%D8%AF%DB%8C%E2%80%8C%D8%A7%D9%81)\n",
    "  استفاده می کنیم.برای اینکار می توانید از [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) کتابخانه sklearn استفاده کنید.\n",
    "  \n",
    "  برای دریافت اسم فیچر ها می توانید از دستور get_feature_names_out روی تبدیل کننده بردار استفاده کنید."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6c72fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the text data into vectors\n",
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48f661dc",
   "metadata": {},
   "source": [
    "# E. Data Preparation (10 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6730568a",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"font-size:16px;\">\n",
    "<font face=\"B Yekan\">\n",
    "برای استفاده از داده ها نیاز است تا دسته هر داده را به شکل عددی ذخیره کنیم.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dc5a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d47f822",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"font-size:16px;\">\n",
    "<font face=\"B Yekan\">\n",
    "متغیر های  پیشبینی و هدف را ایجاد کرده و به شکل مناسب آن ها برای آموزش و تست جدا کنید."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f0e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6abe3cc3",
   "metadata": {},
   "source": [
    "# F. Model Training (80 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9a667fd",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"font-size:16px;\">\n",
    "<font face=\"B Yekan\">\n",
    "با استفاده از روش هایی که تا اینجا یاد گرفتید(SVM , Decision Tree, Random Forest) مدل خود را آموزش دهید و معیار های آن ها را با هم مقایسه کنید.\n",
    "\n",
    "برای روش SVM ماتریس در هم ریختگی را رسم کنید.\n",
    "\n",
    "برای روش درخت تصمیم نمودار درخت خود را در یک فایل pdf خروجی بگیرید.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b81bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, precision_score\n",
    "def results(y_test, y_pred):\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('accuracy_score:', accuracy_score(y_test, y_pred))\n",
    "    print('precision_score:', precision_score(y_test, y_pred))\n",
    "    print('roc_auc_score:', roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152f456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b7d2ee6",
   "metadata": {},
   "source": [
    "# G. Predictor function (10 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eada2522",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"font-size:16px;\">\n",
    "<font face=\"B Yekan\">\n",
    "یک تابع ایجاد کنید که با دریافت مدل ، تبدیل کننده برداری و یک متن به شما بگوید که آیا این پیام هرز است یا خیر.\n",
    "\n",
    "تابع خود را با ورودی های داده شده تست کنید.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e13a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isSpam(model,vectorizer,texts):\n",
    "    \n",
    "    #TODO\n",
    "\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca9a9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=['سلام محمود خوبی؟نگران شدم','این محصول را ازینجا خریدپستی کنید']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2317c6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "ec21d020fdb83eb2829808c0505e5b33037481656f73549d7749c38a50b0ef23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
